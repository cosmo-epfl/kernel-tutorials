{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In this notebook we step through the importing and partitioning of the dataset. For other notebooks, this is done using a utility function found in `../utilities/general.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "\n",
    "# Atomistic structure manipulation\n",
    "from ase.io import read, write\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "# Librascal\n",
    "from rascal.representations import SphericalInvariants as SOAP\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "from utilities.general import FPS, center_matrix, normalize_matrix, load_variables\n",
    "from utilities.kernels import linear_kernel, gaussian_kernel, center_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structures are read using the ASE I/O library from an extended XYZ file, that contains also information on the properties of the structures or the atoms. In this case, we read a property that contains the local chemical shieldings as computed by GIPAW-DFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10\n",
    "input_file=\"../datasets/CSD-1000R.xyz\"\n",
    "properties = [\"CS_local\", \"CS_total\"]\n",
    "    \n",
    "# Read the first N frames of CSD-500\n",
    "frames = read(input_file, index=':{}'.format(N))\n",
    "\n",
    "# Extract chemical shifts\n",
    "for frame in frames:\n",
    "    frame.wrap()\n",
    "\n",
    "Y = np.vstack([np.concatenate([frame.arrays[property] for frame in frames]) for property in properties]).T\n",
    "\n",
    "print(\"Within the {} frames we have {} environments.\".format(N, len(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute SOAP Vectors\n",
    "We use the SOAP power spectrum vectors as atomic descriptors for the structures [(Bart√≥k, 2013)](https://doi.org/10.1103/PhysRevB.87.184115).\n",
    "Understanding SOAP vectors is not necessary for this tutorial, although they are crucial for correlating chemical environments and materials properties. For now, consider the power spectrum SOAP vectors as a three-body correlation function which includes information on each atom, its relationships with neighboring atoms, and the relationships between pairs of neighbors. The correlation function is expanded on a dense basis, and the feature vector contains more information than it is necessary for these tutorials, so we use [farthest point sampling](https://en.wikipedia.org/wiki/Farthest-first_traversal) to only include 200 components of the SOAP vectors while still retaining much of their diversity.\n",
    "\n",
    "SOAP vectors are computed with the librascal package [(librascal GitHub)](https://github.com/cosmo-epfl/librascal). If you don't want (or cannot) install librascal, you can download a precomputed version datafile `precomputed.npz`, that you should store in the `datasets/` folder, as discussed in the [foreword](0_Foreword.ipynb) to this tutorial.  You should then be able to run all tutorials without having to install librascal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_FPS = 200\n",
    "\n",
    "# Compute SOAPs (from librascal tutorial)\n",
    "soap = SOAP(soap_type='PowerSpectrum',\n",
    "           interaction_cutoff=3.5,\n",
    "           max_radial=6,\n",
    "           max_angular=6,\n",
    "           gaussian_sigma_type='Constant',\n",
    "           gaussian_sigma_constant=0.4,\n",
    "           cutoff_smooth_width=0.5)\n",
    "\n",
    "soap_rep = soap.transform(frames)\n",
    "X_raw = soap_rep.get_features(soap)\n",
    "\n",
    "num_features = X_raw.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we prepare a file that can be used to initialize all local variables without having to read the raw data file or using `librascal` to compute the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"../datasets/precomputed.npz\", n_atoms=[], indices=[], X=X_raw, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Each SOAP vector contains {num_features} components.\\\n",
    "       \\nWe use furthest point sampling to generate a subsample of our SOAP vectors.\")\n",
    "\n",
    "# FPS the components\n",
    "col_idxs, col_dist = FPS(X_raw.T, n_FPS)\n",
    "X = X_raw[:, col_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Testing and Training\n",
    "Data is split into a training and testing set, and normalized based on the train set. \n",
    "This makes it easier to compare performance of PCA and linear regression based on the intrinsic\n",
    "variability, and makes the whole analysis dimensionless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits in train and test sets\n",
    "n_train = int(len(Y)/2)\n",
    "n_test = len(Y)-n_train\n",
    "r_train = np.asarray(range(len(Y)))\n",
    "np.random.shuffle(r_train)\n",
    "i_test = list(sorted(r_train[n_train:]))\n",
    "i_train = list(sorted(r_train[:n_train]))\n",
    "\n",
    "X_train = X[i_train]\n",
    "Y_train = Y[i_train]\n",
    "X_test = X[i_test]\n",
    "Y_test = Y[i_test]\n",
    "\n",
    "print(f'Shape of testing data is: {X_train.shape}, ||X|| = {np.linalg.norm(X_train)}.')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centering and Normalizing Data\n",
    "In order to simplify the algebra in what follows, and to treat features and properties on the same grounds, we center and normalize the data. In other words, we calculate the means and standard deviation for the two training arrays (X_train and Y_train) and normalize the other matrices based upon these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the center of the training data\n",
    "X_center = X_train.mean(axis=0)\n",
    "Y_center = Y_train.mean(axis=0)\n",
    "\n",
    "# Center total dataset\n",
    "X = center_matrix(X, center=X_center)\n",
    "Y = center_matrix(Y, center=Y_center)\n",
    "\n",
    "# Center training data\n",
    "X_train = center_matrix(X_train, center=X_center)\n",
    "Y_train = center_matrix(Y_train, center=Y_center)\n",
    "\n",
    "# Center testing data\n",
    "X_test = center_matrix(X_test, center=X_center)\n",
    "Y_test = center_matrix(Y_test, center=Y_center)\n",
    "\n",
    "# Calculate the scale of the training data\n",
    "X_scale = np.linalg.norm(X_train) / np.sqrt(n_train)\n",
    "Y_scale = np.linalg.norm(Y_train) / np.sqrt(n_train)\n",
    "\n",
    "# Scale the total dataset\n",
    "X = normalize_matrix(X, scale=X_scale)\n",
    "Y = normalize_matrix(Y, scale=Y_scale)\n",
    "\n",
    "# Scale the training data\n",
    "X_train = normalize_matrix(X_train, scale=X_scale)\n",
    "Y_train = normalize_matrix(Y_train, scale=Y_scale)\n",
    "\n",
    "# Scale the testing data\n",
    "X_test = normalize_matrix(X_test, scale=X_scale)\n",
    "Y_test = normalize_matrix(Y_test, scale=Y_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Kernels\n",
    "In later notebooks ([Kernel Methods](3_KernelMethods.ipynb) and [Sparse Kernel Methods](4_SparseKernelMethods.ipynb)) we use kernels rather than the raw features. They can be computed as follows using the utility functions and default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train = gaussian_kernel(X_train, X_train)\n",
    "K_train = center_kernel(K_train)\n",
    "\n",
    "K_test = gaussian_kernel(X_test, X_train)\n",
    "K_test = center_kernel(K_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data with the Utility Class\n",
    "\n",
    "The data preparation protocol that is explained in this notebook can be automated using a utility class found in `utilities/general_utils.py`. This call is used in all the example notebooks, and sets all of the variables locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables()\n",
    "locals().update(var_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.933px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
