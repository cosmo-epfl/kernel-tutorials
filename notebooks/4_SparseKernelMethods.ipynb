{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In this notebook, we demonstrate how to adapt the kernel methods shown in the [previous notebook](3_KernelMethods.ipynb) to use sparse kernels.\n",
    "\n",
    "As for the previous notebooks, for each model, we first go step-by-step through the derivation, with equations, embedded links, and citations supplied where useful. At the end of the notebook, we employ a \"Utility Class\" for the model, which is found in the utilities folder and contains all necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "sys.path.append('../')\n",
    "from utilities.general import FPS, load_variables, sorted_eig, get_stats\n",
    "from utilities.plotting import (\n",
    "    plot_projection, plot_regression, check_mirrors, get_cmaps, table_from_dict\n",
    ")\n",
    "from utilities.kernels import linear_kernel, gaussian_kernel, center_kernel\n",
    "from utilities.classes import KPCA, KRR, SparseKPCA, SparseKRR\n",
    "from utilities.classes import KPCovR, SparseKPCovR\n",
    "\n",
    "cmaps = get_cmaps()\n",
    "plt.style.use('../utilities/kernel_pcovr.mplstyle')\n",
    "dbl_fig=(2*plt.rcParams['figure.figsize'][0], plt.rcParams['figure.figsize'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data. For a step-by-step explanation of this, please see [Importing Data](X_ImportingData.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables()\n",
    "locals().update(var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a Sparse Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change this cell to change the kernel function throughout\n",
    "\n",
    "kernel_func = gaussian_kernel\n",
    "kernel_type = 'gaussian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Nystr&ouml;m Approximation\n",
    "\n",
    "In sparse kernel methods, an approximate kernel is used in place of the full kernel. This approximate kernel is typically constructed according to the [Nystr&ouml;m approximation](https://en.wikipedia.org/wiki/Low-rank_matrix_approximations#Nystr%C3%B6m_approximation) [(Williams 2001)](http://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf),\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{K} \\approx \\mathbf{\\hat{K}}_{NN} = \\mathbf{K}_{NM} \\mathbf{K}_{MM}^{-1} \\mathbf{K}_{NM}^T,\n",
    "\\end{equation}\n",
    "\n",
    "Here, $M$ represents a subset of the $N$ total rows/columns of the kernel matrix, i.e. the kernel between a small **active set** that is selected with subsampling method, like farthest point sampling (FPS) [(Eldar 1997)](https://doi.org/10.1109/83.623193), or a CUR decomposition [(Imbalzano2018)](https://doi.org/10.1063/1.5024611), that is discussed in the [next notebook](5_CUR.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In our imported data from `load_variables()`, `X_train` and `X_test` are pre-centered and pre-scaled relative to the train set. Additionally, the imported `K_train` and `K_test` kernels have been constructed using uncentered and unscaled $\\mathbf{X}$ data. If we want to compare the sparse kernels that we will soon construct to the imported \"full\" kernels, we also need to build the sparse kernels on uncentered and unscaled $\\mathbf{X}$ data. Therefore, we undo the scaling and centering on the imported $\\mathbf{X}$ data here, and re-center and re-scale the data after building the sparse kernels. In general, centering and scaling the $\\mathbf{X}$ data before building kernels is optional; however, one should be consistent when working with multiple kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train * X_scale + X_center\n",
    "X_test = X_test * X_scale + X_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_active = 20\n",
    "\n",
    "fps_idxs, _ = FPS(X_train, n_active)\n",
    "Xsparse = X_train[fps_idxs, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, $\\mathbf{K}_{NM}$ is the kernel matrix between input data $\\mathbf{X}$ and $\\mathbf{X_{sparse}}$, a version of $\\mathbf{X}$ containing only the active set. $\\mathbf{K}_{MM}$ is the matrix containing the kernel evaluated between the active set samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmm = kernel_func(Xsparse, Xsparse)\n",
    "Knm_train = kernel_func(X_train, Xsparse)\n",
    "Knm_test = kernel_func(X_test, Xsparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Explicit RKHS\n",
    "\n",
    "Sometimes, it might be more convenient to explicitly write out the projection of the training points\n",
    "on the [RKHS](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space) defined by the active set.\n",
    "This is essentially a KPCA built for the active set, that is not truncated to a few eigenvectors,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\Phi}_{NM} = \\mathbf{K}_{NM} \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}.\n",
    "\\end{equation}\n",
    "\n",
    "Using this definition it is easy to derive the Nyström approximation: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\hat{K}}_{NN} = \\mathbf{\\Phi}_{NM} \\mathbf{\\Phi}_{NM}^T = \n",
    "\\mathbf{K}_{NM}  \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1}  \\mathbf{U}_{MM}^T \\mathbf{K}_{NM}^T\n",
    "= \\mathbf{K}_{NM} \\mathbf{K}_{MM}^{-1} \\mathbf{K}_{NM}^T.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centering the RKHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the \"full\" kernels in [the previous notebook](3_KernelMethods.ipynb) were centered, sparse kernels must also be centered relative to the train set. The goal is to ensure that the Nyström-approximated kernel $\\mathbf{\\hat{K}}_{NN}$ is centered relative to the training set. This is achieved by centering the approximate RKHS features $\\mathbf{\\Phi}_{NM}$, and we denote the centered version of the RKHS features as $\\mathbf{\\tilde{\\Phi}}_{NM} = \\mathbf{\\Phi}_{NM} - \\mathbf{\\bar{\\Phi}}_{M}$. If we represent each element of $\\mathbf{\\Phi}$ in its summation form\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\Phi}_{nm} = \\frac{1}{\\sqrt{\\Lambda_{mm}}}\\sum_{m'}^M \\left(K_{nm'} U_{m'm}\\right), \n",
    "\\end{equation}\n",
    "\n",
    "then the column means are given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\bar{\\Phi}}_{m} = \\frac{1}{\\sqrt{\\Lambda_{mm}}}\\sum_{m'}^M \\left(\\left(\\frac{1}{N}\\sum_n^N K_{nm'}\\right)U_{m'm} \\right), \n",
    "\\end{equation}\n",
    "\n",
    "so the centered feature matrix is computed by $\\mathbf{K}_{NM}$, centered by the column means of the kernel, as denoted by $\\mathbf{\\bar{K}}_M$.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\tilde{\\Phi}}_{NM} =  \\left(\\mathbf{K}_{NM} -\\mathbf{\\bar{K}}_M\\right) \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}.\n",
    "\\end{equation}\n",
    "\n",
    "It is best to keep the column mean $\\mathbf{\\bar{K}}_M$ separate, because it has to be used also when performing out-of-sample embedding, where $\\mathbf{K}_{NM}$ would corresponds to the test set kernel. For consistency, $\\mathbf{\\bar{K}}_M$ must always be the kernel mean associated with the train set.\n",
    "\n",
    "Alternatively, one can store $\\mathbf{\\bar{\\Phi}}_{M}$ and use it for centering.\n",
    "\n",
    "**Note**: in the following we often use $\\mathbf{\\Phi}_{NM}$ and $\\mathbf{\\tilde{\\Phi}}_{NM}$ without the subscripts to indicate the train set features approximated in the active RKHS.\n",
    "\n",
    "<!---\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\tilde{\\Phi}}_{nm} = \\frac{1}{\\sqrt{\\Lambda_{mm}}}\\sum_{m'}^M \\left(\\left(K_{nm'} - \\frac{1}{N}\\sum_{n'}^N K_{n'm'}\\right)U_{m'm}\\right)\n",
    "\\end{equation}\n",
    "\\end{comment}\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel between the active points $\\mathbf{K}_{MM}$ can also be centered independently, though this is optional and can lead to near-zero eigenvalues, as noted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmm = center_kernel(Kmm) # optional\n",
    "\n",
    "K_center = np.mean(Knm_train, axis=0)\n",
    "\n",
    "Knm_train -= K_center\n",
    "Knm_test -= K_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to centering the kernel, one may also want to scale the sparse kernel(s) so that, for example, the trace of the Nyström-approximated train kernel is equal to the number of training points. To achieve this, the sparse kernel $\\mathbf{K}_{NM}$ is divided by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\sqrt{\\operatorname{Tr}(\\mathbf{K}_{NM}\\mathbf{K}_{MM}^{-1}\\mathbf{K}_{NM}^T)}}{n_{train}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{K}_{NM}$ refers to the kernel between the **train set** and the active points. The same scaling should be applied to both the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_scale = np.matmul(Knm_train, np.linalg.pinv(Kmm, rcond=1.0E-12))\n",
    "K_scale = np.matmul(K_scale, Knm_train.T)\n",
    "K_scale = np.sqrt(np.trace(K_scale) / Knm_train.shape[0])\n",
    "\n",
    "Knm_train /= K_scale\n",
    "Knm_test /= K_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computing the RKHS features, it might be wise to discard some of the smaller eigenvalues. For instance, if it has been centered, $\\mathbf{K}_{MM}$ has one _exactly_ zero eigenvalue, and we should take it out of the projection. [(Honeine 2014)](https://arxiv.org/pdf/1407.2904.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmm, Umm = sorted_eig(Kmm, thresh=1e-12)\n",
    "\n",
    "Phi = np.matmul(Knm_train, Umm[:,:n_active-1])\n",
    "Phi = np.matmul(Phi, np.diagflat(1.0/np.sqrt(vmm[0:n_active-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-center and scale the X data\n",
    "X_train = (X_train - X_center) / X_scale\n",
    "X_test = (X_test - X_center) / X_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse KPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse kernel principal component analysis (sKPCA) is formulated in the same way as standard KPCA, with the exception that an approximate kernel matrix is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\tilde{\\Phi}}$ is the feature matrix for the train points in the RKHS defined by the $M$ active set. Sparse KPCA can be understood (and derived) as PCA in the active set RKHS, by computing and diagonalising the covariance matrix built from $\\mathbf{\\Phi}_{NM}$. The covariance should be computed using *centered* kernel features, as discussed above\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C} = \\mathbf{\\tilde{\\Phi}}^T \\mathbf{\\tilde{\\Phi}} = \\mathbf{U}_C \\mathbf{\\Lambda}_C \\mathbf{U}_C^T.\n",
    "\\end{equation}\n",
    "\n",
    "Note that - as usual - one could also compute the RKHS covariance without explicitly diagonalising $\\mathbf{K}_{MM}$, as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C} = \n",
    " \\left(\\mathbf{K}_{NM} -\\mathbf{\\bar{K}}_M\\right) \\mathbf{K}_{MM}^{-1}  \\left(\\mathbf{K}_{NM} -\\mathbf{\\bar{K}}_M\\right)^T,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{\\bar{K}}_M$ indicates the centering vector as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.dot(Phi.T, Phi)\n",
    "\n",
    "v_C, U_C = sorted_eig(C, thresh=1.0E-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting the Sparse KPCA\n",
    "Projecting into latent space, we get\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{T} &= \\mathbf{\\tilde{\\Phi}} \\hat{\\mathbf{U}}_C \\\\\n",
    "    &= \\left(\\mathbf{K}_{NM}- \\bar{\\mathbf{K}}_M\\right)\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2} \\hat{\\mathbf{U}}_C \\\\\n",
    "    &= \\mathbf{K}_{NM}\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2} \\hat{\\mathbf{U}}_C -\\bar{\\mathbf{\\Phi}}\\hat{\\mathbf{U}}_C\\\\\n",
    "    &= \\mathbf{K}_{NM} \\mathbf{P}_{KT} - \\mathbf{\\bar{T}}\n",
    "\\end{align}\n",
    "\n",
    "where our sKPCA projector from kernel space $\\mathbf{P}_{KT} = \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{\\hat{U}}_C$, where $\\mathbf{\\hat{U}}_C$ contains the first $n_{PCA}$ eigenvectors of $\\mathbf{C}$. $\\mathbf{\\bar{T}} = \\bar{\\mathbf{\\Phi}}\\hat{\\mathbf{U}}_C$ centers in the latent space, and is computed and stored for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKT = np.matmul(Umm[:,:n_active-1],\\\n",
    "                    np.diagflat(1.0/np.sqrt(vmm[0:n_active-1])))\n",
    "PKT = np.matmul(PKT, U_C[:, :n_PC])\n",
    "\n",
    "T_train = np.matmul(Knm_train, PKT)\n",
    "T_test = np.matmul(Knm_test, PKT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "ref_kpca = KPCA(n_PC=n_PC, kernel_type=kernel_type)\n",
    "ref_kpca.fit(X_train, K=K_train)\n",
    "xref = ref_kpca.transform(X_test, K=K_test)\n",
    "\n",
    "plot_projection(Y_test, check_mirrors(T_test, xref), fig=fig, ax=axes[0], \\\n",
    "                 title=\"Sparse KPCA on {} Environments\".format(Kmm.shape[0]),\n",
    "                  **cmaps\n",
    "         )\n",
    "plot_projection(Y_test,  xref, fig=fig, ax=axes[1], \\\n",
    "                title=\"KPCA on {} Environments\".format(X_train.shape[0]),\n",
    "               **cmaps\n",
    "         )\n",
    "\n",
    "fig.subplots_adjust(wspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reconstruct $\\mathbf{X}$ using $\\mathbf{P}_{TX} = \\mathbf{\\Lambda}^{-1}\\mathbf{T}^T\\mathbf{X}$, as in KPCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTX = np.matmul(np.diagflat(1.0/(v_C[:n_PC])),np.matmul(T_train.T, X_train))\n",
    "\n",
    "Xr_test = np.matmul(T_test, PTX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Loss\n",
    "\n",
    "The same loss functions are used as in KPCA, so we can compare the loss with that of KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_approx_train = np.matmul(T_train,T_train.T)\n",
    "\n",
    "K_test_test = kernel_func(X_test, X_test)\n",
    "K_test_test = center_kernel(K_test_test)\n",
    "K_approx_test = np.matmul(T_test,T_test.T)\n",
    "\n",
    "table_from_dict([ref_kpca.statistics(X_test, Y_test, K=K_test),\n",
    "                 get_stats(x=X_test, \n",
    "                           xr=Xr_test,\n",
    "                           y=Y_test, \n",
    "                           t=T_test, \n",
    "                           k=K_test_test, \n",
    "                           kapprox=K_approx_test)], \n",
    "                 headers = [\"KPCA\", \"sKPCA\"], \n",
    "                 title=\"sKPCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse KRR\n",
    "\n",
    "## Sparse KRR Weights\n",
    "Let's see how sparsity works out in the case of regression. \n",
    "\n",
    "If we now build a (regularized) linear regression in the RKHS we get the loss\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell = \\lVert \\mathbf{Y} - \\mathbf{\\Phi}\\mathbf{P}_{\\Phi Y} \\rVert^2 + \n",
    "\\lambda \\lVert\\mathbf{P}_{\\Phi Y} \\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "This is solved by \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{\\Phi Y} = \\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}+ \\lambda \\mathbf{I}\\right)^{-1} \\mathbf{\\Phi}^T \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "or, by writing the last $\\mathbf{\\Phi}^T$ in terms of the kernel:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{\\Phi Y} = \\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}+ \\lambda \\mathbf{I}\\right)^{-1} \\mathbf{\\Lambda}_{MM}^{-1/2} \\mathbf{U}_{MM}^T \\mathbf{K}_{NM}^T  \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: Since (kernel) ridge regression is often performed without centering the kernel, we use the uncentered feature matrix $\\mathbf{\\Phi}$ instead of $\\mathbf{\\tilde{\\Phi}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from after we've computed our sparse kernels and recompute $\\mathbf{\\Phi}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vmm, Umm = sorted_eig(Kmm, thresh=0)\n",
    "\n",
    "Phi_raw = np.matmul(Knm_train, Umm[:,:n_active-1])\n",
    "Phi_raw = np.matmul(Phi_raw, np.diagflat(1.0/np.sqrt(vmm[0:n_active-1])))\n",
    "\n",
    "Phi = Knm_train\n",
    "Phi = np.matmul(Knm_train, Umm[:,:n_active-1])\n",
    "Phi = np.matmul(Phi, np.diagflat(1.0/np.sqrt(vmm[0:n_active-1])))\n",
    "\n",
    "PPY = np.matmul(Phi.T, Phi)\n",
    "PPY = PPY + regularization*np.eye(Phi.shape[1])\n",
    "PPY = np.linalg.pinv(PPY)\n",
    "PPY = np.matmul(PPY, Phi.T)\n",
    "PPY = np.matmul(PPY, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Often Cheaper, More Elegant Route\n",
    "\n",
    "We cast this expression into the more commonly used form by a series of simple manipulations that remove the need for diagonalizing $K_{MM}$ and computing $\\mathbf{\\Phi}$. First, we redefine the weights so that \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\Phi}\\mathbf{P}_{\\Phi Y} = \n",
    "\\mathbf{K}_{NM} \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2} \\mathbf{P}_{\\Phi Y} = \n",
    "\\mathbf{K}_{NM} \\tilde{\\mathbf{P}_{K Y}}.\n",
    "\\end{equation}\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{P}_{K Y}}  &= \n",
    "\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{P}_{\\Phi Y} \\\\\n",
    "& = \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}+ \\lambda \\mathbf{I}_M\\right)^{-1} \n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}  \\mathbf{U}_{MM}^T \n",
    "\\mathbf{K}_{NM}^T \\mathbf{Y}\\\\\n",
    "& = \n",
    "\\left(\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{1/2}\\mathbf{\\Phi}^T \\mathbf{\\Phi}\\mathbf{\\Lambda}_{MM}^{1/2}\\mathbf{U}_{MM}^T+ \\lambda \\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}\\mathbf{U}_{MM}^T\\right)^{-1} \n",
    "\\mathbf{K}_{NM}^T \\mathbf{Y}.\n",
    "\\end{align}\n",
    "\n",
    "Now, by noting that \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{1/2} \n",
    "\\mathbf{\\Phi}^T \\mathbf{\\Phi}\n",
    "\\mathbf{\\Lambda}_{MM}^{1/2}  \\mathbf{U}_{MM}^T  = \n",
    "\\mathbf{K}_{NM}^T \\mathbf{K}_{NM},\n",
    "\\end{equation}\n",
    "\n",
    "we see that the sparse KRR model weights is computed by\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{P}_{K Y}}  = \n",
    "\\left(\\mathbf{K}_{NM}^T \\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)^{-1} \n",
    "\\mathbf{K}_{NM}^T \\mathbf{Y}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "PKY = np.matmul(Knm_train.T, Knm_train)\n",
    "PKY = PKY + regularization*Kmm\n",
    "PKY = np.linalg.pinv(PKY)\n",
    "PKY = np.matmul(PKY, Knm_train.T)\n",
    "PKY = np.matmul(PKY, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, this trick provides a (in some cases considerable) speed-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_skrr_train = np.matmul(Knm_train, PKY)\n",
    "Y_skrr_test = np.matmul(Knm_test, PKY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare our results with those from KRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "ref_krr = KRR(regularization=regularization, kernel_type=kernel_type)\n",
    "ref_krr.fit(X=X_train, Y=Y_train, K=K_train)\n",
    "Y_krr = ref_krr.transform(X=X_test, K=K_test)\n",
    "\n",
    "plot_regression(Y_test[:,0], Y_krr[:,0], title=\"KRR\", fig=fig, ax=axes[0], **cmaps)\n",
    "plot_regression(Y_test[:,0], Y_skrr_test[:,0], title=\"Sparse KRR on {} Environments\".format(n_active), fig=fig, ax=axes[1], **cmaps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Loss\n",
    "\n",
    "Here our loss function is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell_{regr} = \\left\\lVert \\mathbf{Y} - \\mathbf{K}_{NM}\\mathbf{P}_{KY}\\right\\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "which we compare with KRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([ref_krr.statistics(X_test, Y_test, K=K_test),\n",
    "                 get_stats(x=X_test, \n",
    "                           y=Y_test, \n",
    "                           yp = Y_skrr_test,\n",
    "                          )], \n",
    "                 headers = [\"KRR\", \"sKRR\"], \n",
    "                 title=\"Ridge Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse KPCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Sparse KPCovR, instead of using the Nystr&ouml;m approximation as in previous sparse methods, we formulate sparse KPCovR from KPCovR in a similar manner to how we derived feature space PCovR from structure space PCovR in the [PCovR Notebook](2_PrincipalCovariatesRegression.ipynb).\n",
    "\n",
    "## A (Very) Quick Recap of Sample and Feature Space PCovR\n",
    "In PCovR, we maximize the similarity\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho = \\operatorname{Tr}\\left(\\tilde{\\mathbf{T}}^T\\mathbf{\\tilde{K}}\\tilde{\\mathbf{T}}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "by taking as our whitened projection $\\tilde{\\mathbf{T}} = \\mathbf{XP}_{X\\tilde{T}}$ the eigenvectors corresponding to the $n_{PCA}$ largest eigenvalues of\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\tilde{K}} = \\alpha {\\mathbf{X} \\mathbf{X}^T}\n",
    "    + (1 - \\alpha) {\\hat{\\mathbf{Y}} \\hat{\\mathbf{Y}}^T},\n",
    "\\end{equation}\n",
    "\n",
    "which combines correlations between the samples in feature and property space. \n",
    "\n",
    "If the number of features is less than the number of samples, we can equivalently rewrite our similarity function as\n",
    "\n",
    "\\begin{align}\n",
    "\\rho &= \\operatorname{Tr}\\left(\\mathbf{P}_{X\\tilde{T}}^T\\mathbf{C}^{1/2}\\mathbf{C}^{-1/2}\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}\\mathbf{C}^{1/2}\\mathbf{C}^{-1/2}\\mathbf{P}_{X\\tilde{T}}\\right)\n",
    "\\end{align}\n",
    "\n",
    "and diagonalize a modified covariance\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{C}} = \\mathbf{C}^{-1/2}\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{C} = \\mathbf{X}^T\\mathbf{X}$ to avoid diagonalizing the $n_{samples} \\times n_{samples}$ matrix $\\tilde{\\mathbf{K}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we just do feature-space PCovR in the RKHS\n",
    "\n",
    "In KPCovR, we maximize the similarity\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho = \\operatorname{Tr}\\left(\\tilde{\\mathbf{T}}^T\\mathbf{\\tilde{K}}\\tilde{\\mathbf{T}}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "however here $\\tilde{\\mathbf{T}} = \\mathbf{KP}_{KT}$.\n",
    "We compute the projection in feature space by maximizing:\n",
    "\n",
    "\\begin{align}\n",
    "\\rho = \\operatorname{Tr}\\left(\\mathbf{P}_{KT}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}_{KT}\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{K} = \\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T$.\n",
    "It would make sense to use $\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}$ as our sparse \"kernel\", but we must insert an identity to ensure that its eigenvectors are orthogonal. We use the covariance, defined as $\\mathbf{C} = \\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{\\Phi}}$, giving:\n",
    "\n",
    "\\begin{align}\n",
    "\\rho=\\operatorname{Tr}\\left(\\mathbf{P}_{KT}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{1/2}\n",
    "\\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{-1/2}\n",
    "\\mathbf{C}^{1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}_{KT}\\right)\n",
    "\\end{align}\n",
    "\n",
    "which ensures orthogonality, as \n",
    "\n",
    "\\begin{align}\n",
    "\\left(\\mathbf{P}_{KT}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{1/2}\n",
    "\\mathbf{C}^{1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}\\right)\n",
    "&=\\left(\\mathbf{P}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}\\right)\\\\\n",
    "&=\\left(\\mathbf{P}^T\\mathbf{K}^T\\mathbf{K}\\mathbf{P}\\right)\\\\\n",
    "&=\\left(\\tilde{\\mathbf{T}}^T\\tilde{\\mathbf{T}}\\right)\\\\\n",
    "&=\\mathbf{I}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.dot(Phi.T, Phi)\n",
    "v_C, U_C = sorted_eig(C, thresh=0)\n",
    "U_C = U_C[:, v_C>0]\n",
    "v_C = v_C[v_C>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In analogy with to feature-space PCovR, we define\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "which evaluates to\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\alpha \\frac{\\mathbf{C}} {\\operatorname{Tr}(\\mathbf{C})/N} + (1 - \\alpha) \\mathbf{C}^{-1/2}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\hat{Y}}\n",
    "\\mathbf{\\hat{Y}}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{-1/2},\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: for consistency, we cannot substitute $\\mathbf{\\hat{Y}}$ with $\\mathbf{Y}_{sKRR} = \\mathbf{K}_{NM}\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)\\mathbf{K}_{NM}^T \\mathbf{Y}$ unless we use a centered feature matrix for sparse KRR. Given that we need to compute the eigenvalue decomposition of $\\mathbf{K}_{MM}$ to orthogonalize the modified covariance, we can instead use the linear regression solution computed directly in active points RKHS: $\\mathbf{Y}_{sKRR} = \\mathbf{\\tilde{\\Phi}}\\left(\\mathbf{C}+ \\lambda \\mathbf{I}\\right)^{-1}\\mathbf{\\tilde{\\Phi}}^T \\mathbf{Y}$.\n",
    "\n",
    "<!---\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\alpha \\frac{\\mathbf{C}} {\\operatorname{Tr}(\\mathbf{C})/N} + (1 - \\alpha) \\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{U}_{MM}^T\n",
    "\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\n",
    "\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)^{-1}\n",
    "\\mathbf{K}_{NM}^T \n",
    "\\mathbf{Y}\n",
    "\\mathbf{Y}^T\n",
    "\\mathbf{K}_{NM}\n",
    "\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)^{-1}\n",
    "\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\n",
    "\\mathbf{U}_{MM}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\alpha \\frac{\\mathbf{C}} {\\operatorname{Tr}(\\mathbf{C})/N} + (1 - \\alpha) \\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{U}_{MM}^T\n",
    "\\left(\\mathbf{I}_{M}+ \\lambda \n",
    "\\mathbf{K}_{MM}\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\\right)^{-1}\n",
    "\\right)^{-1}\n",
    "\\mathbf{K}_{NM}^T \n",
    "\\mathbf{Y}\n",
    "\\mathbf{Y}^T\n",
    "\\mathbf{K}_{NM}\n",
    "\\left(\\mathbf{I}_{M}+ \\lambda \n",
    "\\mathbf{K}_{MM}\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\\right)^{-1}\n",
    "\\right)^{-1}\n",
    "\\mathbf{U}_{MM}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "regularization=1e-6\n",
    "\n",
    "Csqrt = np.matmul(np.matmul(U_C, np.diagflat(np.sqrt(v_C))), U_C.T)\n",
    "iCsqrt = np.matmul(np.matmul(U_C, np.diagflat(1.0/np.sqrt(v_C))), U_C.T)\n",
    "\n",
    "C_pca = C / (np.trace(C)/C.shape[0])\n",
    "\n",
    "C_lr = np.linalg.pinv(C + regularization*np.eye(C.shape[0]))\n",
    "C_lr = np.matmul(Phi, C_lr)\n",
    "C_lr = np.matmul(Phi.T, C_lr)\n",
    "C_lr = np.matmul(iCsqrt, C_lr)\n",
    "C_lr = np.matmul(C_lr, Phi.T)\n",
    "C_lr = np.matmul(C_lr, Y_train.reshape(-1,Y_train.shape[-1]))\n",
    "C_lr = np.matmul(C_lr, C_lr.T)\n",
    "\n",
    "Ct = alpha*C_pca + (1-alpha)*C_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then find the eigendecomposition of \n",
    "$\\mathbf{\\tilde{C}}=\n",
    "\\mathbf{U}_\\mathbf{\\tilde{C}}\\mathbf{\\Lambda}_\\mathbf{\\tilde{C}}\\mathbf{U}_\\mathbf{\\tilde{C}}^T$  and \n",
    "solve for $\\mathbf{P}_{\\tilde{\\Phi} T}$ (again analogous to feature-space PCovR, swapping $\\mathbf{\\Phi}$ for $\\mathbf{X}$): \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{\\tilde{\\Phi} T}=\\mathbf{C}^{-1/2}\\mathbf{\\hat{U}}_\\mathbf{\\tilde{C}}\\mathbf{\\hat{\\Lambda}}_\\mathbf{\\tilde{C}}^{1/2} \n",
    "\\end{equation}\n",
    "\n",
    "where the $\\hat{\\cdot}$ decoration denotes a truncation to $n_{PCA}$ components, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_Ct, U_Ct = sorted_eig(Ct, thresh=0)\n",
    "\n",
    "PPT = np.matmul(iCsqrt, U_Ct[:, :n_PC])\n",
    "PPT = np.matmul(PPT, np.diag(np.sqrt(v_Ct[:n_PC])))\n",
    "v_Ct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting into Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our projection in feature space takes the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\mathbf{\\tilde{\\Phi}}_{NM}\\mathbf{P}_{\\tilde{\\Phi} T}\n",
    "\\end{equation}\n",
    "\n",
    "If we want to project using a kernel rather than a feature space vector, this becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{T} &=  \\left(\\mathbf{K}_{NM}-\\mathbf{\\bar{K}}_M\\right)\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{P}_{\\tilde{\\Phi} T} \\\\\n",
    "&=\\mathbf{K}_{NM}\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{C}^{-1/2}\\mathbf{\\hat{U}}_\\mathbf{\\tilde{C}}\\mathbf{\\hat{\\Lambda}}_\\mathbf{\\tilde{C}}^{1/2} - \\mathbf{\\bar{\\Phi}}\\mathbf{P}_{\\tilde{\\Phi} T} \\\\\n",
    "&= \\mathbf{K}_{NM}\\mathbf{P}_{KT} -  \\mathbf{\\bar{T}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{P}_{K T} = \\mathbf{P}_{K\\Phi} \\mathbf{P}_{\\Phi T} = \n",
    "\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{C}^{-1/2}\\mathbf{\\hat{U}}_\\mathbf{\\tilde{C}}\\mathbf{\\hat{\\Lambda}}_\\mathbf{\\tilde{C}}^{1/2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKT = np.matmul(Umm[:, :n_active-1], np.diagflat(1/np.sqrt(vmm[:n_active-1])))\n",
    "PKT = np.matmul(PKT, PPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T =  np.matmul(Knm_train, PKT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_skpcovr_test = np.matmul(Knm_test, PKT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again compare to the non-sparse kernel version, giving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "ref = KPCovR(alpha=alpha, n_PC=2, kernel_type=kernel_type)\n",
    "ref.fit(X_train, Y_train)\n",
    "xref, yref, r = ref.transform(X_test)\n",
    "\n",
    "plot_projection(Y_test, check_mirrors(T_skpcovr_test, xref), fig=fig, ax=axes[0], title = \"Sparse KPCovR\", **cmaps)\n",
    "plot_projection(Y_test, xref, fig=fig, ax=axes[1], title = \"KPCovR\", **cmaps)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Properties\n",
    "\n",
    "Property prediction takes the exact same form as in KPCovR, except with $\\mathbf{T}$ supplied by our sparse construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTY = np.matmul(T.T, T)\n",
    "PTY = np.linalg.pinv(PTY)\n",
    "PTY = np.matmul(PTY, T.T)\n",
    "PTY = np.matmul(PTY, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred = np.matmul(Knm_test, PKT)\n",
    "Ypred = np.matmul(Ypred, PTY)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_regression(Y_test[:,0], Ypred[:,0], fig=fig, ax=axes[0], title = f\"Sparse KPCovR with {n_active} Environments\", **cmaps)\n",
    "plot_regression(Y_test[:,0], yref[:,0], fig=fig, ax=axes[1], title = \"KPCovR\", **cmaps)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: CUR Decomposition and Feature Selection\n",
    "\n",
    "Continue on to the [next notebook](5_CUR.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Utility Classes\n",
    "\n",
    "Classes from the utility module enable computing SparseKPCA, SparseKRR, and SparseKPCovR with a scikit.learn-like syntax. `SparseKPCovR` is located in `utilities/kpcovr.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.classes import SparseKPCA, SparseKRR\n",
    "from utilities.classes import SparseKPCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: In all sparse kernel classes, the functions `fit`, `transform`, and `statistics` either computes the designated kernels for the supplied $\\mathbf{X}$ data or uses the provided precomputed kernels. The kernel are **always** a keyword argument (e.g. `model.fit(Kmm=Kmm, Knm=Knm)`), and the first argument, positionally, is $\\mathbf{X}$.\n",
    "\n",
    "In each demonstration, we show the function signature using X, but we also supply our precomputed kernel for computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse KPCA with Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skpca = SparseKPCA(n_PC=2, n_active=n_active, kernel_type=kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `skpca.fit(X)` computes the eigendecomposition and internally stores it for projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skpca.fit(X_train, Kmm=Kmm, Knm=Knm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `skpca.transform(X)` computes the projection of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_skpca = skpca.transform(X_test, Knm=Knm_test)\n",
    "\n",
    "plot_projection(Y_test, T_skpca, title=f\"Sparse KPCA on {n_active} Environments\", **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpca.statistics(X)` returns available statistics. Let's compare to KPCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_kpca = KPCA(n_PC=n_PC, kernel_type=kernel_type)\n",
    "ref_kpca.fit(X_train, K=K_train)\n",
    "\n",
    "table_from_dict([ref_kpca.statistics(X_train, K=K_train), \n",
    "                 skpca.statistics(X_train, Knm=Knm_train),\n",
    "                 ref_kpca.statistics(X_test, K=K_test), \n",
    "                 skpca.statistics(X_test, Knm=Knm_test)], \n",
    "                 headers = [\"KPCA (Train)\", \"Sparse KPCA(Train)\",\n",
    "                            \"KPCA (Testing)\", \"Sparse KPCA(Testing)\",\n",
    "                           ], \n",
    "                 title=\"Error in Kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse KRR with Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skrr = SparseKRR(regularization=regularization, n_active=n_active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `skrr.fit(X,Y)` computes the weights $\\mathbf{P}_{KY}$ and internally stores them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skrr.fit(X_train, Y_train, Kmm=Kmm, Knm=Knm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `skrr.transform(X)` computes and return the predicted $\\mathbf{Y}$ values from $\\hat{\\mathbf{Y}}_{SKRR} = \\mathbf{K}\\mathbf{P}_{KY}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_skrr_train = skrr.transform(X_train, Knm=Knm_train)\n",
    "Y_skrr_test = skrr.transform(X_test, Knm=Knm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "ref_krr = KRR(regularization=regularization)\n",
    "ref_krr.fit(X_train, Y_train, K=K_train)\n",
    "Y_krr = ref_krr.transform(X_test, K=K_test)\n",
    "\n",
    "plot_regression(Y_test[:,0], Y_skrr_test[:,0], title=\"Sparse KRR on {} Environments\".format(n_active), fig=fig, ax=axes[0], **cmaps)\n",
    "plot_regression(Y_test[:,0], Y_krr[:,0], title=\"KRR\", fig=fig, ax=axes[1], **cmaps)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `skrr.statistics(X,Y)` outputs the statistics of the regression of $\\mathbf{X}$ and $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We even compare the results of Sparse KRR with our earlier computed KRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([skrr.statistics(X_test, Y_test, Knm=Knm_test), \n",
    "                 ref_krr.statistics(X_test, Y_test, K=K_test)], \n",
    "                 headers = [\"Sparse KRR\", \"KRR\"], \n",
    "                 title=\"KRR Methods: Testing Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse KPCovR from the Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "regularization=1e-6\n",
    "skp = SparseKPCovR(alpha=alpha, n_PC=2, \n",
    "                   n_active=n_active, regularization=regularization, \n",
    "                   kernel_type=kernel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skp.fit(X_train, Y_train, Knm=Knm_train, Kmm=Kmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y, x = skp.transform(X_test, Knm=Knm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_projection(Y_test, t, title = \"Sparse KPCovR\", **cmaps, fig=fig, ax=axes[0])\n",
    "plot_regression(Y_test, y, title = \"Sparse KPCovR\", **cmaps, fig=fig, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_kpcovr = KPCovR(alpha=alpha, regularization=regularization, kernel_type=kernel_type)\n",
    "ref_kpcovr.fit(X_train, Y_train, K=K_train)\n",
    "\n",
    "table_from_dict([skp.statistics(X_test, Y_test, Knm=Knm_test),\n",
    "                ref_kpcovr.statistics(X_test, Y_test, K=K_test)],\n",
    "                headers = [\"Sparse KPCovR\", \"Full KPCovR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.917px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
