{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In this notebook, we introduce kernel-based versions of the models covered in the [Linear Methods](1_LinearMethods.ipynb) and [PCovR](2_PrincipalCovariatesRegression.ipynb) notebooks.\n",
    "\n",
    "As always, for each model, we first go step-by-step through the derivation, with equations, embedded links, and citations supplied where useful. Second, we will employ a \"Utility Class\" for the model, which can be found in the utilities folder and contains all necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "sys.path.append('../')\n",
    "from utilities.general import load_variables, sorted_eig, get_stats\n",
    "from utilities.plotting import (\n",
    "    plot_projection, plot_regression, check_mirrors, get_cmaps, table_from_dict\n",
    ")\n",
    "from utilities.kernels import linear_kernel, gaussian_kernel, center_kernel\n",
    "from utilities.classes import PCA, LR, KPCA, KRR, KPCovR\n",
    "\n",
    "cmaps = get_cmaps()\n",
    "plt.style.use('../utilities/kernel_pcovr.mplstyle')\n",
    "dbl_fig=(2*plt.rcParams['figure.figsize'][0], plt.rcParams['figure.figsize'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data. For a step-by-step explanation of this, please see [Importing Data](X_ImportingData.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables()\n",
    "locals().update(var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Kernels\n",
    "\n",
    "## The Kernel Trick\n",
    "\n",
    "Many kernel methods are similar to linear methods,\n",
    "except that we take advantage of the [**kernel trick**](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick) in order to \n",
    "introduce a non-linear transformation of the feature space.\n",
    "\n",
    "The kernel trick consists in introducing a [**positive definite**](https://en.wikipedia.org/wiki/Positive-definite_kernel) function of pairs of samples, $ k(\\mathbf{x}, \\mathbf{x}^{\\prime})$, computed based on their features, that defines implicitly a higher (possibly infinite) dimensional feature space, in which one can apply linear analysis methods. \n",
    "\n",
    "There are [many](https://en.wikipedia.org/wiki/Positive-definite_kernel#Examples_of_p.d._kernels) positive definite kernels. Common kernels are the linear (dot product) kernel,\n",
    "\\begin{equation}\n",
    "    k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\mathbf{x}^T \\mathbf{x},\n",
    "\\end{equation}\n",
    "and the Gaussian kernel,\n",
    "\\begin{equation}\n",
    "    k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\exp{\\left(-\\gamma\\lVert \\mathbf{x} - \\mathbf{x}^{\\prime}\\rVert^2\\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "In this notebook we use the gaussian kernel (```gaussian_kernel(XA, XB, gamma=1.0)```), however we have also included the linear kernel function (```linear_kernel(XA, XB)```) in ```utilities/kernels.py```. You can check rather easily that if you use a linear kernel all of the kernel methods reduce explicitly to linear regression or principal component analysis in the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the kernels in this cell will change them throughout the notebook tutorial.\n",
    "# The function variable designates the kernel function to use throughout the notebook.\n",
    "# The string variable is used to pass to the utility classes. \n",
    "kernel_func = gaussian_kernel\n",
    "kernel_type = 'gaussian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [representer theorem](https://en.wikipedia.org/wiki/Representer_theorem) guarantees that if the kernel is a positive definite function (both these two examples are) there is a Hilbert space with elements $\\phi(\\mathbf{x})$ whose dot product reproduces the kernel, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\phi(\\mathbf{x})^T\\phi(\\mathbf{x}^{\\prime}).\n",
    "\\end{equation}\n",
    "\n",
    "This Hilbert space is known as the [**reproducing kernel Hilbert space**](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space), or RKHS. If one builds a kernel matrix $\\mathbf{K}$ whose elements contain the kernel computed between the corresponding pair of samples, \n",
    "\n",
    "\\begin{equation}\n",
    "    K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "\\end{equation}\n",
    "\n",
    "the formal relation with the reproducing features can be written in a matrix notation\n",
    "\n",
    "\\begin{equation}\n",
    "\\quad \\mathbf{K} = \\mathbf{\\Phi_X\\Phi_X^T}.\n",
    "\\end{equation}\n",
    "\n",
    "$\\mathbf{\\Phi_X}$ indicates a matrix that holds the values of the RKHS features for each points in $\\mathbf{X}$. To simplify the notation, in this notebook we drop the $\\mathbf{X}$ subscript and refer to this matrix simply as $\\mathbf{\\Phi}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train_raw = kernel_func(X_train, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realize that even if in general $\\phi(\\mathbf{x})$ is not known, _for a given data set_ the representer theorem provides an explicit construction for an approximation of the RKHS. The construction is very closely related to the KPCA method that we discuss below. \n",
    "\n",
    "Start by writing an eigendecomposition of the kernel matrix, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{K} = \\mathbf{U_K} \\mathbf{\\Lambda_K} \\mathbf{U_K}^T.\n",
    "\\end{equation}\n",
    "\n",
    "If one defines \n",
    "\\begin{equation}\n",
    "\\mathbf{\\Phi} =  \\mathbf{U_K} \\mathbf{\\Lambda_K}^{1/2} = \\mathbf{K} \\mathbf{U_K} \\mathbf{\\Lambda_K}^{-1/2}\n",
    "\\end{equation}\n",
    "one sees that $\\mathbf{\\Phi}\\mathbf{\\Phi}^T = \\mathbf{K}$: the eigenvectors of the kernel matrix make it possible to construct a set of features whose scalar product reproduces exactly the values of the kernel for the dataset. \n",
    "\n",
    "The second equality is important because it provides a recipe to build an _approximate_ set of RKHS features for a _new_ set of points. If $\\mathbf{K}_{NN}$ indicates the matrix for the train set, and $\\mathbf{K}_{N'N}$ the matrix that contains the kernels between some new (e.g. test-set) samples and the train samples, one can compute \n",
    "\\begin{equation}\n",
    "\\mathbf{\\Phi}_{N'N} = \\mathbf{K}_{N'N} \\mathbf{U} \\mathbf{\\Lambda}^{-1/2},\n",
    "\\end{equation}\n",
    "where $\\mathbf{U}$ and $\\mathbf{\\Lambda}$ refer to the eigendecomposition of the square $\\mathbf{K}_{NN}$.\n",
    "Then, $\\mathbf{\\Phi}_{N'N}$ is a matrix whose entries approximate the RKHS features for the new set of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_K, U_K = sorted_eig(K_train_raw, thresh=0)\n",
    "\n",
    "Phi = np.matmul(U_K, np.diag(np.sqrt(v_K)))\n",
    "\n",
    "print(np.linalg.norm( (K_train_raw - np.matmul(Phi,Phi.T)) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Centering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as it is often convenient to center features for linear methods, it is often beneficial to work with kernels that are also \"centered\". Centering a kernel can be understood in terms of centering of the associated RKHS features.  \n",
    "\n",
    "Let us first introduce the centering matrix $\\mathbf{1}_{N'N}$, which is just is a $N'\\times N$ matrix containing constant $1/N$ entries. \n",
    "\n",
    "Using the expression above for $\\mathbf{\\Phi}$, one can compute \n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{\\mathbf{\\Phi}}_{N'N} = \\mathbf{1}_{N'N} \\mathbf{K}_{N'N} \\mathbf{U} \\mathbf{\\Lambda}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "A similar centering matrix can be built for $\\mathbf{\\Phi}_{NN}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{\\mathbf{\\Phi}}_{NN} = \\mathbf{1}_{NN} \\mathbf{K}_{NN} \\mathbf{U} \\mathbf{\\Lambda}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "The centered kernel $\\tilde{\\mathbf{K}}_{N'N}$ reads\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{K}}_{N'N} = (\\mathbf{\\Phi}_{N'N} - \\bar{\\mathbf{\\Phi}}_{N'N})(\\mathbf{\\Phi}_{NN}-\\bar{\\mathbf{\\Phi}}_{NN})^T = \n",
    "\\mathbf{K}_{N'N} -  \\mathbf{1}_{N'N} \\mathbf{K}_{NN} -  \\mathbf{K}_{N'N}\\mathbf{1}_{NN}\n",
    "+ \\mathbf{1}_{N'N} \\mathbf{K}_{NN} \\mathbf{1}_{NN}.\n",
    "\\end{equation}\n",
    "\n",
    "which can be computed without the need of ever evaluating explicitly the RKHS features.\n",
    "This is a common pattern in kernel methods: RKHS features allow casting problems in a linear\n",
    "language, but eventually the linear problem can yield an equation in which one only needs to evaluate the kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we center the kernels, which can be done with a utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train = center_kernel(K_train_raw)\n",
    "K_scale = np.trace(K_train) / K_train.shape[0]\n",
    "\n",
    "K_train /= K_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Testing Set Kernel\n",
    "\n",
    "For new data, we must generate a new kernel. Notice that a kernel takes *two* sets of data as arguments, one of which can be our testing $\\mathbf{X}_{N'}$ and the other the training $\\mathbf{X}_N$. This contains the kernels computed between all of the test set samples $N'$ and the train set samples $N$. This can be computed as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_test = kernel_func(X_test, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We center with the training kernel as reference, as discussed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centering relative to the approximate RHKS defined by\n",
    "# the training kernel matrix can be achieved specifying\n",
    "# K_train as the reference kernel matrix\n",
    "\n",
    "K_test = center_kernel(K_test, reference=K_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_test / K_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA (KPCA)\n",
    "\n",
    "In [kernel principal component analysis](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis)\n",
    "(KPCA) we take advantage of the kernel in order to \n",
    "introduce a non-linear transformation of the feature space \n",
    "[(Scholkopf 1996)](http://www.face-rec.org/algorithms/Kernel/kernelPCA_scholkopf.pdf), \n",
    "[(Scholkopf 1998)](http://www.doi.org/10.1162/089976698300017467), and then proceed to single out the largest-variance directions in RKHS. \n",
    "\n",
    "With the linear (dot product) kernel, performing KPCA with a linear kernel is equivalent to performing standard PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principal Components\n",
    "\n",
    "KPCA proceeds analogously to PCA. First, the eigenvalues and eigenvectors of $\\mathbf{K}$ are computed:\n",
    "\\begin{equation}\n",
    "    \\mathbf{K} = \\mathbf{U_K} \\mathbf{\\Lambda_K} \\mathbf{U_K}^T\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_K, U_K = sorted_eig(K_train, thresh=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The KPCA Projection\n",
    "\n",
    "The KPCA projection may then be computed taking the first $n_{PCA}$ components \n",
    "\\begin{equation}\n",
    "    \\mathbf{T} = \\mathbf{K}\\mathbf{P}_{KT} = \\mathbf{K} \\mathbf{\\hat{U}_K} \\mathbf{\\hat{\\Lambda}_K}^{-1/2}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember that in PCA, the projection $\\mathbf{T} = \\mathbf{X}\\mathbf{U}_C$. So why the factor of $\\mathbf{\\Lambda}_K^{-1/2}$?**\n",
    "\n",
    "The eigenvectors of $\\mathbf{K} = \\mathbf{\\Phi}\\mathbf{\\Phi}^T$ are in fact analogous to the eigenvectors of the Gram matrix and thereby related to those of the covariance by $\\mathbf{X}\\mathbf{U_C}$, which is not normalized. In essence, the $\\mathbf{\\Lambda}_K^{-1/2}$ factor serves to normalize our projection. [(Tipping 2001)](https://papers.nips.cc/paper/1791-sparse-kernel-principal-component-analysis.pdf) \n",
    "\n",
    "Note also that the KPCA latent space corresponds to the highest-variance components in the RKHS: if one does not truncate the latent space, $\\mathbf{T}=\\mathbf{\\Phi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_PC = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKT = np.matmul(U_K[:,:n_PC], \n",
    "                np.diagflat(1.0/np.sqrt(v_K[0:n_PC])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression for the testing data is then identical to that for the projection of the train set:\n",
    "\\begin{equation}\n",
    "    \\mathbf{T} = \\mathbf{K}_{N'N} \\mathbf{P}_{KT}\n",
    "\\end{equation}\n",
    "$\\mathbf{P}_{KT}$ is the projection obtained during training, while the $\\mathbf{K}_{N'N}$ indicates the kernel between the test set and the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_KPCA_train = np.matmul(K_train, PKT)\n",
    "T_KPCA_test = np.matmul(K_test, PKT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: if we used a linear kernel, the projection would be identical to the PCA projection (modulo reflection, since the sign of the eigenvectors is not defined), and it would likewise correspond to the Classical MDS solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "plot_projection(Y_train, T_KPCA_train, \n",
    "                fig=fig, ax=axes[0], \n",
    "                title=\"KPCA of Training Data\", **cmaps)\n",
    "plot_projection(Y_test, T_KPCA_test, \n",
    "                fig=fig, ax=axes[1], \n",
    "                title=\"KPCA of Testing Data\", **cmaps)\n",
    "fig.subplots_adjust(wspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is not done very often (because the kernel acts in a different space than the original feature space) it is possible to reconstruct an approximation of the feature vector based on the KPCA latent space. Consider this like \"predicting\" $\\mathbf{X}$, similar to predicting $\\mathbf{Y}$ in linear regression. The projection matrix $\\mathbf{P}_{TX}$ corresponds to the least-square weights\n",
    "    \n",
    "\\begin{equation}    \n",
    "\\mathbf{P}_{TX} = (\\mathbf{T}\\mathbf{T}^T)^{-1}\\mathbf{T}^T \\mathbf{X} =  \\mathbf{\\hat{\\Lambda}}_K^{-1} \\mathbf{T}^T \\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "Where the factor of $\\mathbf{\\Lambda_K}^{-1}$ arises from the fact that\n",
    "$\\mathbf{T}^T\\mathbf{T}=\n",
    "\\mathbf{\\Lambda_K}^{-1/2}\\mathbf{U_K}^T\\mathbf{K}^T\\mathbf{K}\\mathbf{U_K}\\mathbf{\\Lambda_K}^{-1/2}=\n",
    "\\mathbf{\\Lambda_K}^{1/2}\\mathbf{U_K}^T\\mathbf{U_K}\\mathbf{\\Lambda_K}^{1/2}=\n",
    "\\mathbf{\\Lambda_K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTX = np.matmul(np.diagflat(1/((v_K[:n_PC]))) ,            \n",
    "    np.matmul(T_KPCA_train.T, X_train))\n",
    "\n",
    "Xr_train = np.matmul(T_KPCA_train, PTX)\n",
    "Xr_test = np.matmul(T_KPCA_test, PTX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KPCA projection approximates the kernel matrix, so one can check for convergence using the [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_approx_train = np.matmul(T_KPCA_train,T_KPCA_train.T)\n",
    "\n",
    "K_test_test = kernel_func(X_test, X_test)\n",
    "K_test_test = center_kernel(K_test_test)\n",
    "K_approx_test = np.matmul(T_KPCA_test,T_KPCA_test.T)\n",
    "\n",
    "table_from_dict([get_stats(x=X_train, \n",
    "                           xr=Xr_train,\n",
    "                           y=Y_train, \n",
    "                           t=T_KPCA_train, \n",
    "                           k=K_train, \n",
    "                           kapprox=K_approx_train), \n",
    "                 get_stats(x=X_test, \n",
    "                           xr=Xr_test,\n",
    "                           y=Y_test, \n",
    "                           t=T_KPCA_test, \n",
    "                           k=K_test_test, \n",
    "                           kapprox=K_approx_test)], \n",
    "                 headers = [\"Training\", \"Testing\"], \n",
    "                 title=\"KPCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression (KRR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the KRR Weights\n",
    "Just as in KPCA, in kernel ridge regression (KRR) we can use the kernel trick to make property predictions [(Saunders 1998)](https://eprints.soton.ac.uk/258942/1/Dualrr_ICML98.pdf). In KRR, the model takes the form\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{Y}}_{KRR} = \\mathbf{K} \\mathbf{P}_{KY},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{P}_{KY}$ is a vector of regression weights. The regression weights can be computed using\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{P}_{KY} = \\left( \\mathbf{K} + \\lambda \\mathbf{I} \\right)^{-1}\\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ is the regularization parameter \n",
    "[(Girosi 1995)](https://pdfs.semanticscholar.org/96af/5da062cee7ce50fc0624654c61363506772b.pdf),\n",
    "[(Smola 2000)](https://pdfs.semanticscholar.org/dd09/78a594290f6dc530e65983d79a056874185c.pdf), \n",
    "[(Welling)](https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf). This parameter can be set to some small fraction of the maximum eigenvalue $\\lambda_{max}$ of $\\mathbf{K}$, e.g., $\\lambda = 10^{-16}\\lambda_{max}$, or it can be determined through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max = np.amax(np.linalg.eigvalsh(K_train))\n",
    "regularization=1e-4\n",
    "lambda_reg = lambda_max * regularization\n",
    "\n",
    "PKY_krr = np.linalg.solve(K_train + lambda_reg*np.eye(n_train), \n",
    "                          Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our regularization is {{regularization*lambda_max}}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then predict our properties using the training and testing kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_krr_train = np.matmul(K_train, PKY_krr)\n",
    "Y_krr_test = np.matmul(K_test, PKY_krr)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_regression(Y_train[:,0], Y_krr_train[:,0], title=\"Kernel Ridge Regression: Training Set\", fig=fig, ax=axes[0], **cmaps)\n",
    "plot_regression(Y_test[:,0], Y_krr_test[:,0], title=\"Kernel Ridge Regression: Testing Set\", fig=fig, ax=axes[1], **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR as regression in RKHS\n",
    "\n",
    "It is instructive to see how KRR is equivalent to ridge regression in RKHS features. In other words, the equation for ridge regression is equivalent to the equation for _kernel_ ridge regression when $\\mathbf{X}$ is replaced with $\\mathbf{\\Phi}$.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{Y}}_{KRR} = \\mathbf{\\Phi} (\\mathbf{\\Phi}^T\\mathbf{\\Phi} \n",
    "      + \\lambda \\mathbf{I})^{-1} \\mathbf{\\Phi}^T \\mathbf{Y} =\n",
    "   \\mathbf{\\Phi} \\mathbf{\\Phi}^T   (\\mathbf{\\Phi}\\mathbf{\\Phi}^T\n",
    "      + \\lambda \\mathbf{I})^{-1} \\mathbf{Y} = \\mathbf{K} (\\mathbf{K}+\\lambda \\mathbf{I})^{-1} \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "where the second step can be proven through a Taylor expansion, where any function $f(A^T A) * A^T$ can be rewritten as $A^T*f(A A^T)$ by expanding it in the form $\\sum_k c_k (A^T A)^k A^T  = A^T \\sum_k c_k (A A^T)^k$.\n",
    "\n",
    "<!---\n",
    "omes from the relation\n",
    "\\begin{equation}\n",
    "\\left(\\mathbf{B}^T\\mathbf{RB} + \\mathbf{P}^{-1}\\right)^{-1}\\mathbf{B}^T\\mathbf{R}^{-1} = \\mathbf{B}^T \\left(\\mathbf{B}\\mathbf{P}\\mathbf{B}^T + \\mathbf{R}\\right)\n",
    "\\end{equation}\n",
    "whenever $\\mathbf{P}$ and $\\mathbf{R}$ are positive definite. Here, because $\\mathbf{B} = \\mathbf{\\Phi}$, $\\mathbf{P} = \\mathbf{I}$ and $\\mathbf{R} = \\lambda \\mathbf{I}$, the relation holds. [(Matrix Cookbook)](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf), [(Welling)](https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf)\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Loss\n",
    "\n",
    "Here our loss function is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell = \\left\\lVert \\mathbf{Y} - \\mathbf{K}\\mathbf{P}_{KY}\\right\\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "which we can compare to that of ridge regression. Note how the additional flexibility afforded by the non-linear kernel halves the MSE. By reducing the regularization, one could reduce the train set error essentially to zero: in fact, the best regularization needs to be determined by cross-validation, or by using an external test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LR(regularization=regularization)\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "Y_lr = lr.transform(X_test)\n",
    "\n",
    "table_from_dict([get_stats(x=X_test, \n",
    "                           yp=Y_lr,\n",
    "                           y=Y_test, \n",
    "                           ), \n",
    "                 get_stats(x=X_test, \n",
    "                           yp=Y_krr_test,\n",
    "                           y=Y_test, \n",
    "                           k=K_test)], \n",
    "                 headers = [\"LR\", \"KRR\"]\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Regularization Parameter\n",
    "\n",
    "Similar to in linear/ridge regression, including some regularization is equivalent to introducing a $\\mathcal{L}^2$ penalty in the loss, associated to $\\left\\lVert \\mathbf{P}_{KY} \\right\\rVert^2$. Here we express it as a fraction of the maximum eigenvalue. While the regularization is used here just to avoid overfitting, it can also be interpreted -- as it is done in the context of Gaussian process regression -- as a measure of the level of noise that is present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizations = np.linspace(-10, -2, 24)\n",
    "krrs = [KRR(regularization=10**i, kernel_type=kernel_type) \n",
    "        for i in regularizations]\n",
    "\n",
    "for k in krrs:\n",
    "    k.fit(X=X_train, K=K_train, Y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_deter = np.array([k.statistics(X=X_test, \n",
    "                                     Y=Y_test, \n",
    "                                     K=K_test)['Coefficient of Determination<br>($R^2$)'] for k in krrs])\n",
    "best_regularization = 10**regularizations[coeff_deter.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even small regularizations lead to an increase in accuracy for predicting out-of-sample data. The best regularization here is {{round(best_regularization, 8)}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(10**regularizations, coeff_deter, marker='o', zorder=-1)\n",
    "plt.scatter(best_regularization,\n",
    "            max(coeff_deter), marker='o', color='r',\n",
    "           )\n",
    "\n",
    "plt.xlabel(r\"$\\lambda$\")\n",
    "plt.ylabel(r\"$R^2$\")\n",
    "\n",
    "plt.title(r\"Effect of $\\lambda$ on $R^2$ for Kernel Ridge Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCovR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of KPCovR\n",
    "Formulating a kernelized version of PCovR is surprisingly simple. One just needs to write formally the expressions using the RKHS features in lieu of the linear features, and combine them into a kernel matrix to obtain the final expression.\n",
    "\n",
    "As an important reminder, $\\tilde{\\mathbf{K}}$ represents the modified Gram matrix used in PCovR, given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\tilde{\\mathbf{K}} = \\alpha \\mathbf{XX}^T\n",
    "    + (1 - \\alpha) \\hat{\\mathbf{Y}} \\hat{\\mathbf{Y}}^T,\n",
    "\\end{equation}\n",
    "\n",
    "versus $\\mathbf{K}$, which is our kernel matrix.\n",
    "\n",
    "To construct a PCovR based upon the kernel matrix, we first need to use the approximate $\\hat{\\mathbf{Y}}$ obtained from (possibly regularized) KRR:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{Y}} = \\mathbf{K}\\left(\\mathbf{K}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{Y},\n",
    "\\end{equation} \n",
    "\n",
    "and to substitute $\\mathbf{K} = \\mathbf{\\Phi \\Phi}^T$ for $ \\mathbf{XX}^T$. \n",
    "\n",
    "We choose to normalize our kernel matrix dividing by the factor  $\\operatorname{Tr}(\\mathbf{K})/N$, so as to obtain a similar balancing of the LR and PCA parts of the loss as is used for linear PCovR. Thus we finally get\n",
    "\n",
    "\\begin{equation}\n",
    "    \\tilde{\\mathbf{K}} = \\alpha \\frac{\\mathbf{K}} {\\operatorname{Tr}(\\mathbf{K})/N} \n",
    "    + (1 - \\alpha) \\hat{\\mathbf{Y}} \\hat{\\mathbf{Y}}^T.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "regularization = 1e-12\n",
    "\n",
    "krr = KRR(regularization=regularization, kernel_type=kernel_type)\n",
    "krr.fit(X_train, Y_train)\n",
    "Yhat_train = krr.transform(X_train).reshape(-1,Y.shape[-1])\n",
    "PKY = krr.PKY\n",
    "\n",
    "K_pca = K_train/(np.trace(K_train)/n_train)\n",
    "K_lr  = np.matmul(Yhat_train, Yhat_train.T)\n",
    "    \n",
    "Kt = (alpha*K_pca) + (1.0-alpha)*K_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting into Latent Space\n",
    "\n",
    "The projection can be obtained as in the linear PCovR case, combining eigenvalues and eigenvectors of $\\tilde{\\mathbf{K}}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\mathbf{K}\\mathbf{P}_{KT} = \\tilde{\\mathbf{K}}\\mathbf{\\hat{U}_{\\tilde{\\mathbf{K}}}}\\mathbf{\\Lambda_{\\tilde{\\mathbf{K}}}}^{-1/2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_Kt, U_Kt = sorted_eig(Kt, thresh=regularization)\n",
    "\n",
    "# note that Kt might have a large null space\n",
    "print(\"Size {} vs {}\".format(len(v_Kt),len(Kt)))\n",
    "\n",
    "T = np.matmul(Kt, U_Kt[:, :n_PC])\n",
    "T = np.matmul(T, np.diagflat(1.0/np.sqrt(v_Kt[:n_PC])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given each portion of $\\tilde{\\mathbf{K}}$ contains a factor of $\\mathbf{K}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{K}} = \\left(\\alpha \\frac{\\mathbf{K}} {\\operatorname{Tr}(\\mathbf{K})/N} + (1 - \\alpha)\n",
    "\\mathbf{K}\\left(\\mathbf{K}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{YY}^T \\left(\\mathbf{K}+\\mathbf{I}\\lambda\\right)^{-1} \\mathbf{K}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "We can find our projector by dividing $\\tilde{\\mathbf{K}}\\mathbf{\\hat{U}_{\\tilde{\\mathbf{K}}}}\\mathbf{\\Lambda_{\\tilde{\\mathbf{K}}}}^{-1/2}$ by $\\mathbf{K}$, \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{KT} = \\left(\n",
    " \\alpha\\frac{\\mathbf{I}}{\\operatorname{Tr}{(\\mathbf{K})}/N}\n",
    "+ (1 - \\alpha)\n",
    "\\left(\\mathbf{K}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{YY}^T \\left(\\mathbf{K}+\\mathbf{I}\\lambda\\right)^{-1} \\mathbf{K}\n",
    "\\right)  \\mathbf{\\hat{U}_{\\tilde{\\mathbf{K}}}}\\mathbf{\\Lambda_{\\tilde{\\mathbf{K}}}}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{KT} = \\left(\n",
    " \\alpha\\frac{\\mathbf{I}}{\\operatorname{Tr}{(\\mathbf{K})}/N}\n",
    "+ (1 - \\alpha)\n",
    "\\mathbf{P}_{KY}\\mathbf{\\hat{Y}}^T\n",
    "\\right)  \\mathbf{\\hat{U}_{\\tilde{\\mathbf{K}}}}\\mathbf{\\Lambda_{\\tilde{\\mathbf{K}}}}^{-1/2}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_krr = np.matmul(PKY, Yhat_train.T)\n",
    "\n",
    "P_kpca = np.eye(n_train)/(np.trace(K_train)/n_train)\n",
    "    \n",
    "P = (alpha*P_kpca) + (1.0-alpha)*P_krr\n",
    "PKT = np.matmul(P,np.matmul(U_Kt[:,:n_PC], np.diag(1/np.sqrt(v_Kt[:n_PC])) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that this gives the same $\\mathbf{T}$ as above and similarly project the test kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_kpcovr_train = np.matmul(K_train, PKT)\n",
    "T_kpcovr_test = np.matmul(K_test, PKT)\n",
    "\n",
    "print(np.linalg.norm(T-T_kpcovr_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "ref = KPCA(n_PC=2, kernel_type=kernel_type)\n",
    "ref.fit(X_train)\n",
    "xref = ref.transform(X_test)\n",
    "\n",
    "plot_projection(Y_test, T_kpcovr_test, fig=fig, ax=axes[0], \n",
    "                title = r\"KPCovR ($\\alpha={}$)\".format(alpha), **cmaps)\n",
    "plot_projection(Y_test, xref, fig=fig, ax=axes[1], title = \"KPCA\", **cmaps)\n",
    "\n",
    "fig.suptitle(r\"These will become increasingly different as $\\alpha \\to 0.0.$\", \n",
    "             y=0.0, fontsize=plt.rcParams['font.size']+6)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Properties \n",
    "\n",
    "Similar PCovR, to get the KRR prediction we should compute $\\mathbf{P}_{TY}$ such that\n",
    "$\\mathbf{Y} = \\mathbf{K}\\mathbf{P}_{KT}\\mathbf{P}_{TY} = \\mathbf{T} \\mathbf{P}_{TY}$. Recall when we derived linear regression, the optimal weights $\\mathbf{P}_{KY}$ were given by the pseudoinverse of our input variable, subject to some regularization. Using this same logic, $\\mathbf{P}_{TY}$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{TY}=(\\mathbf{T}^T\\mathbf{T})^{-1}\\mathbf{T}^T \\mathbf{Y}=\n",
    "\\mathbf{\\Lambda}_{\\tilde{K}}^{-1}\\mathbf{T}^T \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "**Note:** even though $\\mathbf{K}$ must be computed with all the points in the train set, the \n",
    "prediction uses only the projections in the KPCA space, so the latent KPCovR variables explain fully \n",
    "structure-property relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTY = np.matmul(np.diagflat(1/((v_Kt[:n_PC]))),\n",
    "                np.matmul(T_kpcovr_train.T, Y_train))\n",
    "Ypred = np.matmul(K_test,np.matmul(PKT, PTY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "ref = KRR(regularization=regularization, kernel_type=kernel_type)\n",
    "ref.fit(X_train, Y_train)\n",
    "yref = ref.transform(X_test)\n",
    "\n",
    "errors = np.concatenate((np.abs(Ypred-Y_test),np.abs(yref-Y_test)))\n",
    "\n",
    "plot_regression(Y_test[:,0], Ypred[:,0], fig=fig, ax=axes[0], \n",
    "                vmin=errors.min(), vmax=errors.max(),\n",
    "                title = r\"KPCovR ($\\alpha={}$)\".format(alpha), **cmaps)\n",
    "plot_regression(Y_test[:,0], yref[:,0], fig=fig, ax=axes[1], \n",
    "                vmin=errors.min(), vmax=errors.max(),\n",
    "                title = \"KRR\", **cmaps)\n",
    "\n",
    "fig.suptitle(r\"These will become increasingly different as $\\alpha \\to 1.0.$\", \n",
    "             y=0.0, fontsize=plt.rcParams['font.size']+6)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of KPCovR Performance\n",
    "\n",
    "We will use the utility class to plot how the method is tuned with changing $\\alpha$ and the number of PCA components. Note: executing this cell can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 11\n",
    "alphas = np.linspace(0.0, 1.0, n_alphas)\n",
    "# a = np.linspace(-5.0,5.0,n_alphas)\n",
    "# alphas = np.exp(a)/(np.exp(-a)+np.exp(a))\n",
    "components = np.array([2,3,4,8])\n",
    "n_components = components.size\n",
    "\n",
    "kpcovr_calculators = np.array([[KPCovR(alpha=a, n_PC=c, \n",
    "                                       kernel_type=kernel_type, \n",
    "                                       regularization=1e-2) for a in alphas] \n",
    "                                                        for c in components])\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    for adx, a in enumerate(alphas):\n",
    "        kpcovr_calculators[cdx][adx].fit(X_train, Y_train, K=K_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of KPCovR Projections and Regressions\n",
    "\n",
    "Just like with PCovR, for KPCovR it's useful to get an intuitive sense for the change in the projections and regressions across $\\alpha$, and see the trade-off. Below we plot both projections and regressions for $n_\\alpha$ different values of $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots = int(n_alphas ** 0.5)\n",
    "scale = 3\n",
    "\n",
    "t_ref, y_ref, x_ref = kpcovr_calculators[0][-3].transform(X_test)\n",
    "\n",
    "pfig, pax = plt.subplots(n_plots, int(np.ceil(n_alphas/n_plots)),\n",
    "                                   figsize=(scale*int(np.ceil(n_alphas/n_plots)), scale*n_plots,),\n",
    "                                  )\n",
    "\n",
    "rfig, rax = plt.subplots(n_plots, int(np.ceil(n_alphas/n_plots)),\n",
    "                                   figsize=(scale*int(np.ceil(n_alphas/n_plots)), scale*n_plots,),\n",
    "                                  )\n",
    "for p, r, kpcovr in zip(pax.flatten(), rax.flatten(), kpcovr_calculators[0]):\n",
    "\n",
    "    t,y, x = kpcovr.transform(X_test)\n",
    "    \n",
    "    plot_projection(Y_test, check_mirrors(t, t_ref), fig=pfig, ax=p, **cmaps, alpha=1.0)\n",
    "    \n",
    "    plot_regression(Y_test[:,0], y[:,0], fig=pfig, ax=r, **cmaps, alpha=1.0)\n",
    "    \n",
    "    p.set_title(r\"$\\alpha=$\"+str(round(kpcovr.alpha,6)))\n",
    "    r.set_title(r\"$\\alpha=$\"+str(round(kpcovr.alpha,6)))\n",
    "\n",
    "    \n",
    "for p, r in zip(pax.flatten()[n_alphas:], rax.flatten()[n_alphas:]):\n",
    "    p.axis('off')\n",
    "    r.axis('off')\n",
    "    \n",
    "pfig.subplots_adjust(wspace=0.6, hspace=0.6)\n",
    "pfig.suptitle(r\"Projections across $\\alpha$\")\n",
    "rfig.subplots_adjust(wspace=0.6, hspace=0.6)\n",
    "rfig.suptitle(r\"Regressions across $\\alpha$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of KPCovR Loss Terms\n",
    "\n",
    "Computing the loss minimised by KPCovR is trivial if one computes explicitly a RKHS approximation of $\\mathbf{\\Phi}$, but it requires some work if one wants to avoid evaluating $\\mathbf{\\Phi}$\n",
    "\n",
    "Rewriting in terms of the RKHS, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\ell = \\alpha\\lVert \\mathbf{\\Phi} - \\mathbf{\\Phi}\\mathbf{P}_{\\Phi T}\\mathbf{P}_{T\\Phi}\\rVert^2 + (1 - \\alpha)\\left\\lVert \\mathbf{Y} - \\mathbf{KP}_{KT}\\mathbf{P}_{TY}\\right\\rVert^2.\n",
    "\\end{equation}\n",
    "\n",
    "In case one wants to avoid evaluating the RKHS, however, $\\ell_\\text{proj}$ may be computed in terms of the kernel.\n",
    "\n",
    "Indicating the kernel between set $A$ and $B$ as $\\mathbf{K}_{AB}$, the projection of set $A$ as $\\mathbf{T}_A$, and with N and V as the train and validation/test set, one obtains\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\ell_\\text{proj}=&\n",
    "\\operatorname{Tr}\\left[\n",
    "\\mathbf{K}_{VV} - 2\n",
    "    \\mathbf{K}_{VN} \\mathbf{T}_N (\\mathbf{T}_N^T \\mathbf{T}_N)^{-1}  \\mathbf{T}_V^T\\right.\\\\\n",
    "    +&\\mathbf{T}_V(\\mathbf{T}_N^T \\mathbf{T}_N)^{-1}  \\mathbf{T}_N^T   \\mathbf{K}_{NN} \\left.\\mathbf{T}_N (\\mathbf{T}_N^T \\mathbf{T}_N)^{-1}    \\mathbf{T}_V^T .\n",
    "\\right]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "When the loss is evaluated on the train set, so that $N\\equiv V$, this expression reduces to\n",
    "\\begin{equation}\n",
    "      \\ell_\\text{proj} = \\operatorname{Tr}\\left(\\mathbf{K}_{NN} - \\mathbf{K}_{NN} \\mathbf{P}_{KT} \\mathbf{P}_{TK} \\right).\n",
    "\\end{equation}\n",
    "where $\\mathbf{P}_{TK} = (\\mathbf{T}_N^T \\mathbf{T}_N)^{-1} \\mathbf{T}_N^T \\mathbf{K}_{NN}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " kpcovr_calculators[3][3].loss(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_proj = np.zeros((n_components, n_alphas))\n",
    "L_regr = np.zeros((n_components, n_alphas))\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    for adx, a in enumerate(alphas):\n",
    "        L_proj[cdx, adx], L_regr[cdx, adx] = kpcovr_calculators[cdx][adx].loss(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [axs_regr, axs_proj] = plt.subplots(1, 2, figsize=(12,6), sharex=True)\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    axs_regr.plot(alphas, L_regr[cdx, :], marker='o', label='{:d} PCs'.format(c))\n",
    "    axs_proj.plot(alphas,L_proj[cdx, :], marker='o', label='{:d} PCs'.format(c))\n",
    "\n",
    "axs_regr.set_ylabel(r'$\\ell_{regr}$')\n",
    "axs_regr.set_xlabel(r'$\\alpha$')\n",
    "axs_proj.set_ylabel(r'$\\ell_{proj}$')\n",
    "axs_proj.set_xlabel(r'$\\alpha$')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "axs_regr.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing KPCovR, the total error is reduced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,6))\n",
    "axsLoss = fig.add_subplot(1, 2, 1)\n",
    "axsSum = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    axsLoss.loglog(L_proj[cdx, :], L_regr[cdx, :], marker='o', label='{:d} PCs'.format(c))\n",
    "\n",
    "axsLoss.set_xlabel(r'$\\ell_{proj}$')\n",
    "axsLoss.set_ylabel(r'$\\ell_{regr}$')\n",
    "axsLoss.legend()\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    loss_sum = L_regr[cdx, :] + L_proj[cdx, :]\n",
    "    axsSum.semilogy(alphas, loss_sum, marker='o', label='{:d} PCs'.format(c))\n",
    "    print('Optimal alpha for {:d} PCs = {:.2f}'.format(c, alphas[np.argmin(loss_sum)]))\n",
    "    \n",
    "axsSum.set_xlabel(r'$\\alpha$')\n",
    "axsSum.set_ylabel(r'$\\ell_{regr} + \\ell_{proj}$')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Sparse Kernel Methods\n",
    "\n",
    "Continue on to the [next notebook](4_SparseKernelMethods.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Utility Classes\n",
    "\n",
    "Classes from the utility module enable computing KPCA, KRR, and KPCovR with a scikit.learn-like syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.classes import KPCA, KRR, KPCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: In all kernel classes, the functions `fit`, `transform`, and `statistics` will either compute the designated kernel for the supplied $\\mathbf{X}$ data or use the provided precomputed kernel. The kernel is **always** a keyword argument (e.g. `model.fit(K=K)`), and the first argument, positionally, is $\\mathbf{X}$.\n",
    "\n",
    "In each demonstration, we will show the function signature using X, but we will also supply our precomputed kernel for computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KPCA(n_PC=2, kernel_type=kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `kpca.fit(X)` will generate the kernel for $\\mathbf{X}$ and compute/internally store the eigenvectors/values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca.fit(X_train, K=K_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `kpca.transform(X)` will compute and return the KPCA projection $\\mathbf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_KPCA_train = kpca.transform(X_train, K=K_train)\n",
    "T_KPCA_test= kpca.transform(X_test, K=K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_projection(Y_train, T_KPCA_train, title=\"KPCA of Training Data\", fig=fig, ax=axes[0], **cmaps)\n",
    "plot_projection(Y_test, T_KPCA_test, title=\"KPCA of Testing Data\", fig=fig, ax=axes[1], **cmaps)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `kpca.statistics(X, Y)` will output the statistics of the projection of the kernel created from $\\mathbf{X}$ and the regression onto $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([kpca.statistics(X_train, Y_train, K=K_train), \n",
    "                 kpca.statistics(X_test, Y_test, K=K_test)], \n",
    "                 headers = [\"Training\", \"Testing\"], \n",
    "                 title=\"KPCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr = KRR(regularization=regularization, kernel_type=kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `krr.fit(X,Y)` will compute the weights $\\mathbf{P}_{KY}$ and internally store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr.fit(X_train, Y_train, K=K_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `krr.transform(X)` will generate the desired kernel from input $\\mathbf{X}$ and compute and return the predicted $\\mathbf{Y}$ values from $\\hat{\\mathbf{Y}}_{KRR} = \\mathbf{K}\\mathbf{P}_{KY}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_krr_train = krr.transform(X_train, K=K_train)\n",
    "Y_krr_test = krr.transform(X_test, K=K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_regression(Y_train[:,0], Y_krr_train[:,0], title=\"Kernel Ridge Regression of Training Data\", fig=fig, ax=axes[0], **cmaps)\n",
    "plot_regression(Y_test[:,0], Y_krr_test[:,0], title=\"Kernel Ridge Regression of Testing Data\", fig=fig, ax=axes[1], **cmaps)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `krr.statistics(X,Y)` will output the statistics of the regression of $\\mathbf{X}$ and $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([krr.statistics(X=X_train, Y=Y_train, K=K_train), \n",
    "                 krr.statistics(X=X_test, Y=Y_test, K=K_test)], \n",
    "                 headers = [\"Training\", \"Testing\"], \n",
    "                 title=\"Kernel Ridge Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = KPCovR(alpha=0.5, n_PC=2, kernel_type=kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `kpcovr.fit(X,Y)` will compute the weights $\\mathbf{P}_{KY}$ and internally store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp.fit(X_train, Y_train, K=K_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `kpcovr.transform(X)` will return the latent space transformation $\\mathbf{T}$, the predicted properties, and the reconstructed input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,y,x = kp.transform(X_test, K=K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_projection(Y_test, t, title = \"KPCovR\", fig=fig, ax=ax[0], **cmaps)\n",
    "plot_regression(Y_test[:,0], y[:,0], title = \"KPCovR\", fig=fig, ax=ax[1], **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For KPCovR, we can compare statistics with previous kernel methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr = KPCovR(alpha=0.0, n_PC=2, kernel_type=kernel_type)\n",
    "krr.fit(X_train, Y_train, K=K_train)\n",
    "kpca = KPCovR(alpha=1.0, n_PC=2, kernel_type=kernel_type)\n",
    "kpca.fit(X_train, Y_train, K=K_train)\n",
    "\n",
    "table_from_dict([kp.statistics(X_test, Y_test, K=K_test), \\\n",
    "                 krr.statistics(X_test, Y_test, K=K_test),\n",
    "                 kpca.statistics(X_test, Y_test, K=K_test)\n",
    "                ], \\\n",
    "                headers = [r\"KPCovR ($\\alpha={}$)\".format(kp.alpha), \"KRR\", \"KPCA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
